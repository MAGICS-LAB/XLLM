{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jys3649/miniconda3/envs/xllm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name_or_path = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto'\n",
    "        ).eval()\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token = tokenizer.unk_token if tokenizer.pad_token is None else tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template =  \"[INST] <<SYS>><</SYS>>\\n\\n{instruction} [/INST] \"\n",
    "input_test = \"Hello, I am a chatbot. What is your name?\"\n",
    "input_test = template.format(instruction=input_test)\n",
    "input_sample = tokenizer(input_test, return_tensors='pt')\n",
    "input_ids = input_sample['input_ids'].cuda()\n",
    "attention_mask = input_sample['attention_mask'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "model.layers.22.mlp.gate_proj.weight\n",
      "model.layers.22.mlp.up_proj.weight\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "model.layers.23.mlp.gate_proj.weight\n",
      "model.layers.23.mlp.up_proj.weight\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "model.layers.24.mlp.gate_proj.weight\n",
      "model.layers.24.mlp.up_proj.weight\n",
      "model.layers.24.mlp.down_proj.weight\n",
      "model.layers.24.input_layernorm.weight\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "model.layers.25.mlp.gate_proj.weight\n",
      "model.layers.25.mlp.up_proj.weight\n",
      "model.layers.25.mlp.down_proj.weight\n",
      "model.layers.25.input_layernorm.weight\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "model.layers.26.mlp.gate_proj.weight\n",
      "model.layers.26.mlp.up_proj.weight\n",
      "model.layers.26.mlp.down_proj.weight\n",
      "model.layers.26.input_layernorm.weight\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "model.layers.27.mlp.gate_proj.weight\n",
      "model.layers.27.mlp.up_proj.weight\n",
      "model.layers.27.mlp.down_proj.weight\n",
      "model.layers.27.input_layernorm.weight\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "model.layers.28.self_attn.q_proj.weight\n",
      "model.layers.28.self_attn.k_proj.weight\n",
      "model.layers.28.self_attn.v_proj.weight\n",
      "model.layers.28.self_attn.o_proj.weight\n",
      "model.layers.28.mlp.gate_proj.weight\n",
      "model.layers.28.mlp.up_proj.weight\n",
      "model.layers.28.mlp.down_proj.weight\n",
      "model.layers.28.input_layernorm.weight\n",
      "model.layers.28.post_attention_layernorm.weight\n",
      "model.layers.29.self_attn.q_proj.weight\n",
      "model.layers.29.self_attn.k_proj.weight\n",
      "model.layers.29.self_attn.v_proj.weight\n",
      "model.layers.29.self_attn.o_proj.weight\n",
      "model.layers.29.mlp.gate_proj.weight\n",
      "model.layers.29.mlp.up_proj.weight\n",
      "model.layers.29.mlp.down_proj.weight\n",
      "model.layers.29.input_layernorm.weight\n",
      "model.layers.29.post_attention_layernorm.weight\n",
      "model.layers.30.self_attn.q_proj.weight\n",
      "model.layers.30.self_attn.k_proj.weight\n",
      "model.layers.30.self_attn.v_proj.weight\n",
      "model.layers.30.self_attn.o_proj.weight\n",
      "model.layers.30.mlp.gate_proj.weight\n",
      "model.layers.30.mlp.up_proj.weight\n",
      "model.layers.30.mlp.down_proj.weight\n",
      "model.layers.30.input_layernorm.weight\n",
      "model.layers.30.post_attention_layernorm.weight\n",
      "model.layers.31.self_attn.q_proj.weight\n",
      "model.layers.31.self_attn.k_proj.weight\n",
      "model.layers.31.self_attn.v_proj.weight\n",
      "model.layers.31.self_attn.o_proj.weight\n",
      "model.layers.31.mlp.gate_proj.weight\n",
      "model.layers.31.mlp.up_proj.weight\n",
      "model.layers.31.mlp.down_proj.weight\n",
      "model.layers.31.input_layernorm.weight\n",
      "model.layers.31.post_attention_layernorm.weight\n",
      "model.norm.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "# print the layer name for each layer in the llama model\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.embed_tokens\n",
      "model.layers\n",
      "model.layers.0\n",
      "model.layers.0.self_attn\n",
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.0.self_attn.o_proj\n",
      "model.layers.0.self_attn.rotary_emb\n",
      "model.layers.0.mlp\n",
      "model.layers.0.mlp.gate_proj\n",
      "model.layers.0.mlp.up_proj\n",
      "model.layers.0.mlp.down_proj\n",
      "model.layers.0.mlp.act_fn\n",
      "model.layers.0.input_layernorm\n",
      "model.layers.0.post_attention_layernorm\n",
      "model.layers.1\n",
      "model.layers.1.self_attn\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.1.self_attn.o_proj\n",
      "model.layers.1.self_attn.rotary_emb\n",
      "model.layers.1.mlp\n",
      "model.layers.1.mlp.gate_proj\n",
      "model.layers.1.mlp.up_proj\n",
      "model.layers.1.mlp.down_proj\n",
      "model.layers.1.mlp.act_fn\n",
      "model.layers.1.input_layernorm\n",
      "model.layers.1.post_attention_layernorm\n",
      "model.layers.2\n",
      "model.layers.2.self_attn\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.2.self_attn.o_proj\n",
      "model.layers.2.self_attn.rotary_emb\n",
      "model.layers.2.mlp\n",
      "model.layers.2.mlp.gate_proj\n",
      "model.layers.2.mlp.up_proj\n",
      "model.layers.2.mlp.down_proj\n",
      "model.layers.2.mlp.act_fn\n",
      "model.layers.2.input_layernorm\n",
      "model.layers.2.post_attention_layernorm\n",
      "model.layers.3\n",
      "model.layers.3.self_attn\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.3.self_attn.o_proj\n",
      "model.layers.3.self_attn.rotary_emb\n",
      "model.layers.3.mlp\n",
      "model.layers.3.mlp.gate_proj\n",
      "model.layers.3.mlp.up_proj\n",
      "model.layers.3.mlp.down_proj\n",
      "model.layers.3.mlp.act_fn\n",
      "model.layers.3.input_layernorm\n",
      "model.layers.3.post_attention_layernorm\n",
      "model.layers.4\n",
      "model.layers.4.self_attn\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.4.self_attn.o_proj\n",
      "model.layers.4.self_attn.rotary_emb\n",
      "model.layers.4.mlp\n",
      "model.layers.4.mlp.gate_proj\n",
      "model.layers.4.mlp.up_proj\n",
      "model.layers.4.mlp.down_proj\n",
      "model.layers.4.mlp.act_fn\n",
      "model.layers.4.input_layernorm\n",
      "model.layers.4.post_attention_layernorm\n",
      "model.layers.5\n",
      "model.layers.5.self_attn\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.5.self_attn.o_proj\n",
      "model.layers.5.self_attn.rotary_emb\n",
      "model.layers.5.mlp\n",
      "model.layers.5.mlp.gate_proj\n",
      "model.layers.5.mlp.up_proj\n",
      "model.layers.5.mlp.down_proj\n",
      "model.layers.5.mlp.act_fn\n",
      "model.layers.5.input_layernorm\n",
      "model.layers.5.post_attention_layernorm\n",
      "model.layers.6\n",
      "model.layers.6.self_attn\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.6.self_attn.o_proj\n",
      "model.layers.6.self_attn.rotary_emb\n",
      "model.layers.6.mlp\n",
      "model.layers.6.mlp.gate_proj\n",
      "model.layers.6.mlp.up_proj\n",
      "model.layers.6.mlp.down_proj\n",
      "model.layers.6.mlp.act_fn\n",
      "model.layers.6.input_layernorm\n",
      "model.layers.6.post_attention_layernorm\n",
      "model.layers.7\n",
      "model.layers.7.self_attn\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.7.self_attn.o_proj\n",
      "model.layers.7.self_attn.rotary_emb\n",
      "model.layers.7.mlp\n",
      "model.layers.7.mlp.gate_proj\n",
      "model.layers.7.mlp.up_proj\n",
      "model.layers.7.mlp.down_proj\n",
      "model.layers.7.mlp.act_fn\n",
      "model.layers.7.input_layernorm\n",
      "model.layers.7.post_attention_layernorm\n",
      "model.layers.8\n",
      "model.layers.8.self_attn\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.8.self_attn.o_proj\n",
      "model.layers.8.self_attn.rotary_emb\n",
      "model.layers.8.mlp\n",
      "model.layers.8.mlp.gate_proj\n",
      "model.layers.8.mlp.up_proj\n",
      "model.layers.8.mlp.down_proj\n",
      "model.layers.8.mlp.act_fn\n",
      "model.layers.8.input_layernorm\n",
      "model.layers.8.post_attention_layernorm\n",
      "model.layers.9\n",
      "model.layers.9.self_attn\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.9.self_attn.o_proj\n",
      "model.layers.9.self_attn.rotary_emb\n",
      "model.layers.9.mlp\n",
      "model.layers.9.mlp.gate_proj\n",
      "model.layers.9.mlp.up_proj\n",
      "model.layers.9.mlp.down_proj\n",
      "model.layers.9.mlp.act_fn\n",
      "model.layers.9.input_layernorm\n",
      "model.layers.9.post_attention_layernorm\n",
      "model.layers.10\n",
      "model.layers.10.self_attn\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.10.self_attn.o_proj\n",
      "model.layers.10.self_attn.rotary_emb\n",
      "model.layers.10.mlp\n",
      "model.layers.10.mlp.gate_proj\n",
      "model.layers.10.mlp.up_proj\n",
      "model.layers.10.mlp.down_proj\n",
      "model.layers.10.mlp.act_fn\n",
      "model.layers.10.input_layernorm\n",
      "model.layers.10.post_attention_layernorm\n",
      "model.layers.11\n",
      "model.layers.11.self_attn\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.11.self_attn.o_proj\n",
      "model.layers.11.self_attn.rotary_emb\n",
      "model.layers.11.mlp\n",
      "model.layers.11.mlp.gate_proj\n",
      "model.layers.11.mlp.up_proj\n",
      "model.layers.11.mlp.down_proj\n",
      "model.layers.11.mlp.act_fn\n",
      "model.layers.11.input_layernorm\n",
      "model.layers.11.post_attention_layernorm\n",
      "model.layers.12\n",
      "model.layers.12.self_attn\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.12.self_attn.o_proj\n",
      "model.layers.12.self_attn.rotary_emb\n",
      "model.layers.12.mlp\n",
      "model.layers.12.mlp.gate_proj\n",
      "model.layers.12.mlp.up_proj\n",
      "model.layers.12.mlp.down_proj\n",
      "model.layers.12.mlp.act_fn\n",
      "model.layers.12.input_layernorm\n",
      "model.layers.12.post_attention_layernorm\n",
      "model.layers.13\n",
      "model.layers.13.self_attn\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.13.self_attn.o_proj\n",
      "model.layers.13.self_attn.rotary_emb\n",
      "model.layers.13.mlp\n",
      "model.layers.13.mlp.gate_proj\n",
      "model.layers.13.mlp.up_proj\n",
      "model.layers.13.mlp.down_proj\n",
      "model.layers.13.mlp.act_fn\n",
      "model.layers.13.input_layernorm\n",
      "model.layers.13.post_attention_layernorm\n",
      "model.layers.14\n",
      "model.layers.14.self_attn\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.14.self_attn.o_proj\n",
      "model.layers.14.self_attn.rotary_emb\n",
      "model.layers.14.mlp\n",
      "model.layers.14.mlp.gate_proj\n",
      "model.layers.14.mlp.up_proj\n",
      "model.layers.14.mlp.down_proj\n",
      "model.layers.14.mlp.act_fn\n",
      "model.layers.14.input_layernorm\n",
      "model.layers.14.post_attention_layernorm\n",
      "model.layers.15\n",
      "model.layers.15.self_attn\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.15.self_attn.o_proj\n",
      "model.layers.15.self_attn.rotary_emb\n",
      "model.layers.15.mlp\n",
      "model.layers.15.mlp.gate_proj\n",
      "model.layers.15.mlp.up_proj\n",
      "model.layers.15.mlp.down_proj\n",
      "model.layers.15.mlp.act_fn\n",
      "model.layers.15.input_layernorm\n",
      "model.layers.15.post_attention_layernorm\n",
      "model.layers.16\n",
      "model.layers.16.self_attn\n",
      "model.layers.16.self_attn.q_proj\n",
      "model.layers.16.self_attn.k_proj\n",
      "model.layers.16.self_attn.v_proj\n",
      "model.layers.16.self_attn.o_proj\n",
      "model.layers.16.self_attn.rotary_emb\n",
      "model.layers.16.mlp\n",
      "model.layers.16.mlp.gate_proj\n",
      "model.layers.16.mlp.up_proj\n",
      "model.layers.16.mlp.down_proj\n",
      "model.layers.16.mlp.act_fn\n",
      "model.layers.16.input_layernorm\n",
      "model.layers.16.post_attention_layernorm\n",
      "model.layers.17\n",
      "model.layers.17.self_attn\n",
      "model.layers.17.self_attn.q_proj\n",
      "model.layers.17.self_attn.k_proj\n",
      "model.layers.17.self_attn.v_proj\n",
      "model.layers.17.self_attn.o_proj\n",
      "model.layers.17.self_attn.rotary_emb\n",
      "model.layers.17.mlp\n",
      "model.layers.17.mlp.gate_proj\n",
      "model.layers.17.mlp.up_proj\n",
      "model.layers.17.mlp.down_proj\n",
      "model.layers.17.mlp.act_fn\n",
      "model.layers.17.input_layernorm\n",
      "model.layers.17.post_attention_layernorm\n",
      "model.layers.18\n",
      "model.layers.18.self_attn\n",
      "model.layers.18.self_attn.q_proj\n",
      "model.layers.18.self_attn.k_proj\n",
      "model.layers.18.self_attn.v_proj\n",
      "model.layers.18.self_attn.o_proj\n",
      "model.layers.18.self_attn.rotary_emb\n",
      "model.layers.18.mlp\n",
      "model.layers.18.mlp.gate_proj\n",
      "model.layers.18.mlp.up_proj\n",
      "model.layers.18.mlp.down_proj\n",
      "model.layers.18.mlp.act_fn\n",
      "model.layers.18.input_layernorm\n",
      "model.layers.18.post_attention_layernorm\n",
      "model.layers.19\n",
      "model.layers.19.self_attn\n",
      "model.layers.19.self_attn.q_proj\n",
      "model.layers.19.self_attn.k_proj\n",
      "model.layers.19.self_attn.v_proj\n",
      "model.layers.19.self_attn.o_proj\n",
      "model.layers.19.self_attn.rotary_emb\n",
      "model.layers.19.mlp\n",
      "model.layers.19.mlp.gate_proj\n",
      "model.layers.19.mlp.up_proj\n",
      "model.layers.19.mlp.down_proj\n",
      "model.layers.19.mlp.act_fn\n",
      "model.layers.19.input_layernorm\n",
      "model.layers.19.post_attention_layernorm\n",
      "model.layers.20\n",
      "model.layers.20.self_attn\n",
      "model.layers.20.self_attn.q_proj\n",
      "model.layers.20.self_attn.k_proj\n",
      "model.layers.20.self_attn.v_proj\n",
      "model.layers.20.self_attn.o_proj\n",
      "model.layers.20.self_attn.rotary_emb\n",
      "model.layers.20.mlp\n",
      "model.layers.20.mlp.gate_proj\n",
      "model.layers.20.mlp.up_proj\n",
      "model.layers.20.mlp.down_proj\n",
      "model.layers.20.mlp.act_fn\n",
      "model.layers.20.input_layernorm\n",
      "model.layers.20.post_attention_layernorm\n",
      "model.layers.21\n",
      "model.layers.21.self_attn\n",
      "model.layers.21.self_attn.q_proj\n",
      "model.layers.21.self_attn.k_proj\n",
      "model.layers.21.self_attn.v_proj\n",
      "model.layers.21.self_attn.o_proj\n",
      "model.layers.21.self_attn.rotary_emb\n",
      "model.layers.21.mlp\n",
      "model.layers.21.mlp.gate_proj\n",
      "model.layers.21.mlp.up_proj\n",
      "model.layers.21.mlp.down_proj\n",
      "model.layers.21.mlp.act_fn\n",
      "model.layers.21.input_layernorm\n",
      "model.layers.21.post_attention_layernorm\n",
      "model.layers.22\n",
      "model.layers.22.self_attn\n",
      "model.layers.22.self_attn.q_proj\n",
      "model.layers.22.self_attn.k_proj\n",
      "model.layers.22.self_attn.v_proj\n",
      "model.layers.22.self_attn.o_proj\n",
      "model.layers.22.self_attn.rotary_emb\n",
      "model.layers.22.mlp\n",
      "model.layers.22.mlp.gate_proj\n",
      "model.layers.22.mlp.up_proj\n",
      "model.layers.22.mlp.down_proj\n",
      "model.layers.22.mlp.act_fn\n",
      "model.layers.22.input_layernorm\n",
      "model.layers.22.post_attention_layernorm\n",
      "model.layers.23\n",
      "model.layers.23.self_attn\n",
      "model.layers.23.self_attn.q_proj\n",
      "model.layers.23.self_attn.k_proj\n",
      "model.layers.23.self_attn.v_proj\n",
      "model.layers.23.self_attn.o_proj\n",
      "model.layers.23.self_attn.rotary_emb\n",
      "model.layers.23.mlp\n",
      "model.layers.23.mlp.gate_proj\n",
      "model.layers.23.mlp.up_proj\n",
      "model.layers.23.mlp.down_proj\n",
      "model.layers.23.mlp.act_fn\n",
      "model.layers.23.input_layernorm\n",
      "model.layers.23.post_attention_layernorm\n",
      "model.layers.24\n",
      "model.layers.24.self_attn\n",
      "model.layers.24.self_attn.q_proj\n",
      "model.layers.24.self_attn.k_proj\n",
      "model.layers.24.self_attn.v_proj\n",
      "model.layers.24.self_attn.o_proj\n",
      "model.layers.24.self_attn.rotary_emb\n",
      "model.layers.24.mlp\n",
      "model.layers.24.mlp.gate_proj\n",
      "model.layers.24.mlp.up_proj\n",
      "model.layers.24.mlp.down_proj\n",
      "model.layers.24.mlp.act_fn\n",
      "model.layers.24.input_layernorm\n",
      "model.layers.24.post_attention_layernorm\n",
      "model.layers.25\n",
      "model.layers.25.self_attn\n",
      "model.layers.25.self_attn.q_proj\n",
      "model.layers.25.self_attn.k_proj\n",
      "model.layers.25.self_attn.v_proj\n",
      "model.layers.25.self_attn.o_proj\n",
      "model.layers.25.self_attn.rotary_emb\n",
      "model.layers.25.mlp\n",
      "model.layers.25.mlp.gate_proj\n",
      "model.layers.25.mlp.up_proj\n",
      "model.layers.25.mlp.down_proj\n",
      "model.layers.25.mlp.act_fn\n",
      "model.layers.25.input_layernorm\n",
      "model.layers.25.post_attention_layernorm\n",
      "model.layers.26\n",
      "model.layers.26.self_attn\n",
      "model.layers.26.self_attn.q_proj\n",
      "model.layers.26.self_attn.k_proj\n",
      "model.layers.26.self_attn.v_proj\n",
      "model.layers.26.self_attn.o_proj\n",
      "model.layers.26.self_attn.rotary_emb\n",
      "model.layers.26.mlp\n",
      "model.layers.26.mlp.gate_proj\n",
      "model.layers.26.mlp.up_proj\n",
      "model.layers.26.mlp.down_proj\n",
      "model.layers.26.mlp.act_fn\n",
      "model.layers.26.input_layernorm\n",
      "model.layers.26.post_attention_layernorm\n",
      "model.layers.27\n",
      "model.layers.27.self_attn\n",
      "model.layers.27.self_attn.q_proj\n",
      "model.layers.27.self_attn.k_proj\n",
      "model.layers.27.self_attn.v_proj\n",
      "model.layers.27.self_attn.o_proj\n",
      "model.layers.27.self_attn.rotary_emb\n",
      "model.layers.27.mlp\n",
      "model.layers.27.mlp.gate_proj\n",
      "model.layers.27.mlp.up_proj\n",
      "model.layers.27.mlp.down_proj\n",
      "model.layers.27.mlp.act_fn\n",
      "model.layers.27.input_layernorm\n",
      "model.layers.27.post_attention_layernorm\n",
      "model.layers.28\n",
      "model.layers.28.self_attn\n",
      "model.layers.28.self_attn.q_proj\n",
      "model.layers.28.self_attn.k_proj\n",
      "model.layers.28.self_attn.v_proj\n",
      "model.layers.28.self_attn.o_proj\n",
      "model.layers.28.self_attn.rotary_emb\n",
      "model.layers.28.mlp\n",
      "model.layers.28.mlp.gate_proj\n",
      "model.layers.28.mlp.up_proj\n",
      "model.layers.28.mlp.down_proj\n",
      "model.layers.28.mlp.act_fn\n",
      "model.layers.28.input_layernorm\n",
      "model.layers.28.post_attention_layernorm\n",
      "model.layers.29\n",
      "model.layers.29.self_attn\n",
      "model.layers.29.self_attn.q_proj\n",
      "model.layers.29.self_attn.k_proj\n",
      "model.layers.29.self_attn.v_proj\n",
      "model.layers.29.self_attn.o_proj\n",
      "model.layers.29.self_attn.rotary_emb\n",
      "model.layers.29.mlp\n",
      "model.layers.29.mlp.gate_proj\n",
      "model.layers.29.mlp.up_proj\n",
      "model.layers.29.mlp.down_proj\n",
      "model.layers.29.mlp.act_fn\n",
      "model.layers.29.input_layernorm\n",
      "model.layers.29.post_attention_layernorm\n",
      "model.layers.30\n",
      "model.layers.30.self_attn\n",
      "model.layers.30.self_attn.q_proj\n",
      "model.layers.30.self_attn.k_proj\n",
      "model.layers.30.self_attn.v_proj\n",
      "model.layers.30.self_attn.o_proj\n",
      "model.layers.30.self_attn.rotary_emb\n",
      "model.layers.30.mlp\n",
      "model.layers.30.mlp.gate_proj\n",
      "model.layers.30.mlp.up_proj\n",
      "model.layers.30.mlp.down_proj\n",
      "model.layers.30.mlp.act_fn\n",
      "model.layers.30.input_layernorm\n",
      "model.layers.30.post_attention_layernorm\n",
      "model.layers.31\n",
      "model.layers.31.self_attn\n",
      "model.layers.31.self_attn.q_proj\n",
      "model.layers.31.self_attn.k_proj\n",
      "model.layers.31.self_attn.v_proj\n",
      "model.layers.31.self_attn.o_proj\n",
      "model.layers.31.self_attn.rotary_emb\n",
      "model.layers.31.mlp\n",
      "model.layers.31.mlp.gate_proj\n",
      "model.layers.31.mlp.up_proj\n",
      "model.layers.31.mlp.down_proj\n",
      "model.layers.31.mlp.act_fn\n",
      "model.layers.31.input_layernorm\n",
      "model.layers.31.post_attention_layernorm\n",
      "model.norm\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def attach_hooks(model, kind='mlp'):\n",
    "    act_dict = OrderedDict()\n",
    "\n",
    "    def _make_hook(name):\n",
    "        def _hook(mod, inp, out):\n",
    "            act_dict[name] = out[0]\n",
    "\n",
    "        return _hook\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if kind == 'mlp':\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                module.register_forward_hook(_make_hook(name))\n",
    "        elif kind == 'attn':\n",
    "            if isinstance(module, torch.nn.MultiheadAttention):\n",
    "                module.register_forward_hook(_make_hook(name))\n",
    "        elif kind == 'all':\n",
    "            module.register_forward_hook(_make_hook(name))\n",
    "        else:\n",
    "            raise ValueError('Unknown kind')\n",
    "    return act_dict\n",
    "\n",
    "hooked_model = attach_hooks(model, kind='all')\n",
    "output = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens torch.Size([33, 4096])\n",
      "model.layers.0.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.0.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.0.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.0.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.0.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.0.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.0.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.0.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.0.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.0.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.0.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.0.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.0.mlp torch.Size([33, 4096])\n",
      "model.layers.0 torch.Size([1, 33, 4096])\n",
      "model.layers.1.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.1.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.1.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.1.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.1.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.1.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.1.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.1.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.1.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.1.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.1.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.1.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.1.mlp torch.Size([33, 4096])\n",
      "model.layers.1 torch.Size([1, 33, 4096])\n",
      "model.layers.2.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.2.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.2.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.2.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.2.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.2.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.2.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.2.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.2.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.2.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.2.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.2.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.2.mlp torch.Size([33, 4096])\n",
      "model.layers.2 torch.Size([1, 33, 4096])\n",
      "model.layers.3.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.3.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.3.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.3.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.3.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.3.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.3.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.3.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.3.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.3.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.3.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.3.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.3.mlp torch.Size([33, 4096])\n",
      "model.layers.3 torch.Size([1, 33, 4096])\n",
      "model.layers.4.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.4.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.4.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.4.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.4.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.4.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.4.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.4.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.4.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.4.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.4.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.4.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.4.mlp torch.Size([33, 4096])\n",
      "model.layers.4 torch.Size([1, 33, 4096])\n",
      "model.layers.5.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.5.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.5.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.5.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.5.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.5.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.5.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.5.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.5.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.5.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.5.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.5.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.5.mlp torch.Size([33, 4096])\n",
      "model.layers.5 torch.Size([1, 33, 4096])\n",
      "model.layers.6.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.6.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.6.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.6.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.6.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.6.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.6.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.6.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.6.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.6.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.6.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.6.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.6.mlp torch.Size([33, 4096])\n",
      "model.layers.6 torch.Size([1, 33, 4096])\n",
      "model.layers.7.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.7.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.7.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.7.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.7.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.7.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.7.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.7.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.7.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.7.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.7.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.7.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.7.mlp torch.Size([33, 4096])\n",
      "model.layers.7 torch.Size([1, 33, 4096])\n",
      "model.layers.8.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.8.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.8.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.8.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.8.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.8.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.8.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.8.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.8.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.8.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.8.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.8.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.8.mlp torch.Size([33, 4096])\n",
      "model.layers.8 torch.Size([1, 33, 4096])\n",
      "model.layers.9.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.9.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.9.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.9.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.9.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.9.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.9.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.9.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.9.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.9.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.9.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.9.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.9.mlp torch.Size([33, 4096])\n",
      "model.layers.9 torch.Size([1, 33, 4096])\n",
      "model.layers.10.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.10.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.10.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.10.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.10.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.10.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.10.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.10.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.10.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.10.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.10.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.10.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.10.mlp torch.Size([33, 4096])\n",
      "model.layers.10 torch.Size([1, 33, 4096])\n",
      "model.layers.11.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.11.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.11.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.11.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.11.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.11.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.11.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.11.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.11.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.11.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.11.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.11.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.11.mlp torch.Size([33, 4096])\n",
      "model.layers.11 torch.Size([1, 33, 4096])\n",
      "model.layers.12.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.12.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.12.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.12.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.12.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.12.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.12.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.12.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.12.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.12.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.12.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.12.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.12.mlp torch.Size([33, 4096])\n",
      "model.layers.12 torch.Size([1, 33, 4096])\n",
      "model.layers.13.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.13.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.13.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.13.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.13.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.13.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.13.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.13.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.13.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.13.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.13.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.13.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.13.mlp torch.Size([33, 4096])\n",
      "model.layers.13 torch.Size([1, 33, 4096])\n",
      "model.layers.14.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.14.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.14.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.14.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.14.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.14.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.14.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.14.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.14.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.14.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.14.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.14.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.14.mlp torch.Size([33, 4096])\n",
      "model.layers.14 torch.Size([1, 33, 4096])\n",
      "model.layers.15.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.15.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.15.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.15.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.15.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.15.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.15.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.15.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.15.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.15.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.15.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.15.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.15.mlp torch.Size([33, 4096])\n",
      "model.layers.15 torch.Size([1, 33, 4096])\n",
      "model.layers.16.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.16.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.16.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.16.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.16.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.16.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.16.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.16.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.16.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.16.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.16.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.16.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.16.mlp torch.Size([33, 4096])\n",
      "model.layers.16 torch.Size([1, 33, 4096])\n",
      "model.layers.17.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.17.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.17.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.17.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.17.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.17.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.17.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.17.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.17.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.17.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.17.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.17.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.17.mlp torch.Size([33, 4096])\n",
      "model.layers.17 torch.Size([1, 33, 4096])\n",
      "model.layers.18.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.18.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.18.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.18.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.18.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.18.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.18.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.18.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.18.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.18.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.18.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.18.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.18.mlp torch.Size([33, 4096])\n",
      "model.layers.18 torch.Size([1, 33, 4096])\n",
      "model.layers.19.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.19.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.19.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.19.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.19.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.19.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.19.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.19.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.19.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.19.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.19.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.19.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.19.mlp torch.Size([33, 4096])\n",
      "model.layers.19 torch.Size([1, 33, 4096])\n",
      "model.layers.20.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.20.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.20.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.20.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.20.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.20.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.20.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.20.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.20.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.20.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.20.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.20.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.20.mlp torch.Size([33, 4096])\n",
      "model.layers.20 torch.Size([1, 33, 4096])\n",
      "model.layers.21.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.21.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.21.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.21.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.21.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.21.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.21.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.21.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.21.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.21.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.21.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.21.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.21.mlp torch.Size([33, 4096])\n",
      "model.layers.21 torch.Size([1, 33, 4096])\n",
      "model.layers.22.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.22.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.22.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.22.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.22.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.22.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.22.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.22.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.22.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.22.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.22.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.22.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.22.mlp torch.Size([33, 4096])\n",
      "model.layers.22 torch.Size([1, 33, 4096])\n",
      "model.layers.23.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.23.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.23.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.23.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.23.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.23.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.23.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.23.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.23.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.23.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.23.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.23.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.23.mlp torch.Size([33, 4096])\n",
      "model.layers.23 torch.Size([1, 33, 4096])\n",
      "model.layers.24.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.24.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.24.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.24.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.24.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.24.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.24.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.24.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.24.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.24.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.24.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.24.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.24.mlp torch.Size([33, 4096])\n",
      "model.layers.24 torch.Size([1, 33, 4096])\n",
      "model.layers.25.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.25.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.25.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.25.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.25.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.25.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.25.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.25.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.25.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.25.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.25.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.25.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.25.mlp torch.Size([33, 4096])\n",
      "model.layers.25 torch.Size([1, 33, 4096])\n",
      "model.layers.26.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.26.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.26.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.26.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.26.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.26.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.26.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.26.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.26.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.26.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.26.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.26.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.26.mlp torch.Size([33, 4096])\n",
      "model.layers.26 torch.Size([1, 33, 4096])\n",
      "model.layers.27.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.27.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.27.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.27.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.27.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.27.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.27.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.27.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.27.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.27.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.27.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.27.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.27.mlp torch.Size([33, 4096])\n",
      "model.layers.27 torch.Size([1, 33, 4096])\n",
      "model.layers.28.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.28.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.28.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.28.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.28.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.28.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.28.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.28.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.28.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.28.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.28.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.28.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.28.mlp torch.Size([33, 4096])\n",
      "model.layers.28 torch.Size([1, 33, 4096])\n",
      "model.layers.29.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.29.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.29.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.29.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.29.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.29.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.29.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.29.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.29.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.29.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.29.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.29.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.29.mlp torch.Size([33, 4096])\n",
      "model.layers.29 torch.Size([1, 33, 4096])\n",
      "model.layers.30.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.30.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.30.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.30.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.30.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.30.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.30.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.30.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.30.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.30.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.30.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.30.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.30.mlp torch.Size([33, 4096])\n",
      "model.layers.30 torch.Size([1, 33, 4096])\n",
      "model.layers.31.input_layernorm torch.Size([33, 4096])\n",
      "model.layers.31.self_attn.q_proj torch.Size([33, 4096])\n",
      "model.layers.31.self_attn.k_proj torch.Size([33, 4096])\n",
      "model.layers.31.self_attn.v_proj torch.Size([33, 4096])\n",
      "model.layers.31.self_attn.rotary_emb torch.Size([33, 128])\n",
      "model.layers.31.self_attn.o_proj torch.Size([33, 4096])\n",
      "model.layers.31.self_attn torch.Size([1, 33, 4096])\n",
      "model.layers.31.post_attention_layernorm torch.Size([33, 4096])\n",
      "model.layers.31.mlp.gate_proj torch.Size([33, 11008])\n",
      "model.layers.31.mlp.act_fn torch.Size([33, 11008])\n",
      "model.layers.31.mlp.up_proj torch.Size([33, 11008])\n",
      "model.layers.31.mlp.down_proj torch.Size([33, 4096])\n",
      "model.layers.31.mlp torch.Size([33, 4096])\n",
      "model.layers.31 torch.Size([1, 33, 4096])\n",
      "model.norm torch.Size([33, 4096])\n",
      "model torch.Size([1, 33, 4096])\n",
      "lm_head torch.Size([33, 32000])\n",
      " torch.Size([1, 33, 32000])\n"
     ]
    }
   ],
   "source": [
    "for name, x in hooked_model.items():\n",
    "    print(name, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 33])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m      4\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmosaicml/mpt-7b-chat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m config\u001b[38;5;241m.\u001b[39mattn_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_impl\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtriton\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m config\u001b[38;5;241m.\u001b[39minit_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# For fast initialization directly on GPU!\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xllm/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:1120\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[1;32m   1119\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1120\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(pretrained_model_name_or_path):\n\u001b[1;32m   1124\u001b[0m         config_class\u001b[38;5;241m.\u001b[39mregister_for_auto_class()\n",
      "File \u001b[0;32m~/miniconda3/envs/xllm/lib/python3.8/site-packages/transformers/dynamic_module_utils.py:500\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[1;32m    488\u001b[0m final_module \u001b[38;5;241m=\u001b[39m get_cached_module_file(\n\u001b[1;32m    489\u001b[0m     repo_id,\n\u001b[1;32m    490\u001b[0m     module_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    499\u001b[0m )\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xllm/lib/python3.8/site-packages/transformers/dynamic_module_utils.py:200\u001b[0m, in \u001b[0;36mget_class_in_module\u001b[0;34m(class_name, module_path)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mImport a module on the cache directory for modules and extract a class from it.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    `typing.Type`: The class looked for.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m module_path \u001b[38;5;241m=\u001b[39m module_path\u001b[38;5;241m.\u001b[39mreplace(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 200\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/xllm/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:783\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-chat/1a1d410c70591fcc1a46486a254cd0e600e7b1b4/configuration_mpt.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Optional, Union\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_alibi_support, is_flash_v1_installed, is_flash_v2_installed\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attn_config_defaults\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FC_CLASS_REGISTRY\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-chat/1a1d410c70591fcc1a46486a254cd0e600e7b1b4/attention.py:60\u001b[0m\n\u001b[1;32m     57\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m hidden[:, :, :, \u001b[38;5;28;01mNone\u001b[39;00m, :]\u001b[38;5;241m.\u001b[39mexpand(b, s, kv_n_heads, n_rep, d)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden\u001b[38;5;241m.\u001b[39mreshape(b, s, kv_n_heads \u001b[38;5;241m*\u001b[39m n_rep, d)\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscaled_multihead_dot_product_attention\u001b[39m(query: torch\u001b[38;5;241m.\u001b[39mTensor, key: torch\u001b[38;5;241m.\u001b[39mTensor, value: torch\u001b[38;5;241m.\u001b[39mTensor, n_heads: \u001b[38;5;28mint\u001b[39m, kv_n_heads: \u001b[38;5;28mint\u001b[39m, past_key_value: Optional[\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m]\u001b[49m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, softmax_scale: Optional[\u001b[38;5;28mfloat\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attn_bias: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key_padding_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_causal: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dropout_p: \u001b[38;5;28mfloat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, training: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, needs_weights: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[\u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]]]:\n\u001b[1;32m     61\u001b[0m     q \u001b[38;5;241m=\u001b[39m rearrange(query, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb s (h d) -> b h s d\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39mn_heads)\n\u001b[1;32m     62\u001b[0m     k \u001b[38;5;241m=\u001b[39m rearrange(key, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb s (h d) -> b h d s\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39mkv_n_heads)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "name = 'mosaicml/mpt-7b-chat'\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\n",
    "config.attn_config['attn_impl'] = 'triton'\n",
    "config.init_device = 'cuda:0' # For fast initialization directly on GPU!\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  name,\n",
    "  config=config,\n",
    "  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n",
    "  trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
